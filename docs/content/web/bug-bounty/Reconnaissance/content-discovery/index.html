<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Content Discovery Script</title>
<style>
  body { font-family: Arial, sans-serif; line-height: 1.6; max-width: 900px; margin: 2em auto; padding: 0 1em; }
  h1, h2 { color: #2a7ae2; }
  code { background: #f4f4f4; padding: 0.2em 0.4em; border-radius: 3px; }
  pre { background: #f9f9f9; border-left: 4px solid #2a7ae2; padding: 1em; overflow-x: auto; white-space: pre-wrap; }
  ul { margin-top: 0; }
  .tool { margin-bottom: 1.5em; }
  button {
    cursor: pointer;
    margin-top: 10px;
    background-color: #2a7ae2;
    border: none;
    color: white;
    padding: 8px 14px;
    border-radius: 5px;
    font-size: 1em;
  }
</style>
</head>
<body>

<h1>üìö Content Discovery Script (robots.txt & sitemap.xml)</h1>

<p>This Python script helps automate content discovery on a target domain by fetching and parsing <code>robots.txt</code> and <code>sitemap.xml</code> files, checking URL statuses, and saving results. It is useful for reconnaissance in web security assessments.</p>

<hr />

<div class="tool">
  <h2>Python Script: Content Discovery (robots.txt & sitemap.xml)</h2>
  <button onclick="toggleScript()" aria-expanded="false" aria-controls="pythonScript">
    ‚ñ∂ Show Python Script
  </button>
  <pre id="pythonScript" style="display:none;">
#!/usr/bin/env python3

import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from termcolor import cprint, colored

def fetch_robots_txt(domain):
    url = urljoin(domain, "/robots.txt")
    cprint(f"\n[*] Fetching robots.txt from {url}", "cyan")
    try:
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            cprint(f"[-] robots.txt not found (status {response.status_code})", "yellow")
            return []
        paths = []
        for line in response.text.splitlines():
            if line.lower().startswith("disallow:"):
                path = line.split(":")[1].strip()
                if path:
                    paths.append(path)
        return paths
    except Exception as e:
        cprint(f"[-] Error fetching robots.txt: {e}", "red")
        return []

def fetch_sitemap_urls(domain):
    url = urljoin(domain, "/sitemap.xml")
    cprint(f"\n[*] Fetching sitemap.xml from {url}", "cyan")
    try:
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            cprint(f"[-] sitemap.xml not found (status {response.status_code})", "yellow")
            return []
        soup = BeautifulSoup(response.text, "xml")
        return [loc.text.strip() for loc in soup.find_all("loc")]
    except Exception as e:
        cprint(f"[-] Error fetching sitemap.xml: {e}", "red")
        return []

def check_url_status(full_url):
    try:
        response = requests.head(full_url, timeout=10, allow_redirects=True)
        return response.status_code
    except Exception:
        return "Error"

def main():
    cprint("\n=== üîç Content Discovery (robots.txt & sitemap.xml) ===", "green", attrs=["bold"])
    domain = input("\nüåê Enter the target domain (e.g., https://example.com): ").strip()
    if not domain.startswith("http"):
        domain = "https://" + domain

    discovered_urls = []

    # Robots.txt
    robots_paths = fetch_robots_txt(domain)
    if robots_paths:
        cprint(f"\n[+] Found {len(robots_paths)} disallowed paths in robots.txt:", "green")
        for path in robots_paths:
            full_url = urljoin(domain, path)
            status = check_url_status(full_url)
            color = "green" if status == 200 else "yellow" if status == 403 else "red"
            cprint(f"  [ {status} ] {full_url}", color)
            discovered_urls.append((full_url, status))
    else:
        cprint("[-] No disallowed paths found in robots.txt.", "yellow")

    # Sitemap.xml
    sitemap_urls = fetch_sitemap_urls(domain)
    if sitemap_urls:
        cprint(f"\n[+] Found {len(sitemap_urls)} URLs in sitemap.xml:", "green")
        for url in sitemap_urls:
            status = check_url_status(url)
            color = "green" if status == 200 else "yellow" if status == 403 else "red"
            cprint(f"  [ {status} ] {url}", color)
            discovered_urls.append((url, status))
    else:
        cprint("[-] No URLs found in sitemap.xml.", "yellow")

    # Save results
    if discovered_urls:
        with open("discovered_content.txt", "w") as f:
            for url, status in discovered_urls:
                f.write(f"{status} {url}\n")
        cprint(f"\nüìÅ Results saved to discovered_content.txt\n", "cyan")
    else:
        cprint("\n[!] No URLs discovered to save.\n", "yellow")

if __name__ == "__main__":
    main()
  </pre>
</div>

<script>
function toggleScript() {
  const scriptBlock = document.getElementById('pythonScript');
  const btn = event.target;
  if (scriptBlock.style.display === 'none') {
    scriptBlock.style.display = 'block';
    btn.textContent = '‚ñº Hide Python Script';
    btn.setAttribute('aria-expanded', 'true');
  } else {
    scriptBlock.style.display = 'none';
    btn.textContent = '‚ñ∂ Show Python Script';
    btn.setAttribute('aria-expanded', 'false');
  }
}
</script>

<hr />

<h2>Summary</h2>
<ul>
  <li>Fetch and parse <code>robots.txt</code> to find disallowed paths.</li>
  <li>Parse <code>sitemap.xml</code> to extract site URLs.</li>
  <li>Check HTTP status codes for discovered URLs to verify availability.</li>
  <li>Save all discovered URLs and statuses to <code>discovered_content.txt</code> for later analysis.</li>
  <li>Use this script during reconnaissance to uncover hidden or important content on target websites.</li>
</ul>

<p>Happy content hunting! üïµÔ∏è‚Äç‚ôÇÔ∏èüîç</p>

<div style="margin-top: 2em;">
  <a href="javascript:history.back()" style="text-decoration: none; font-weight: bold; font-size: 1.1em;">
    &#8592; Go Back
  </a>
</div>

</body>
</html>
